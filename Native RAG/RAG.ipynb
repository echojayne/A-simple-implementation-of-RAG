{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get PDF Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pdfplumber\n",
    "import os\n",
    "os.chdir('RAG')\n",
    "print(\"Current Working Directory:\", os.getcwd()) # Mark sure your current working directory is in /path/to/RAG\n",
    "import pdfplumber # type: ignore\n",
    "\n",
    "path = 'dataset/general_notes.pdf'\n",
    "with pdfplumber.open(path) as pdf: \n",
    "    content = ''\n",
    "    for i in range(len(pdf.pages)):\n",
    "        page = pdf.pages[i] \n",
    "        page_content = '\\n'.join(page.extract_text().split('\\n')[:-1])\n",
    "        content = content + page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering invalid characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_invalid_characters(text, valid_chars):\n",
    "    \"\"\"Filter out characters that are not in the valid character set.\"\"\"\n",
    "    vilid_characters = ''.join([char for char in text if char in valid_chars])\n",
    "    filtered_characters = ''.join([char for char in text if char not in vilid_characters])\n",
    "    return vilid_characters, filtered_characters\n",
    "\n",
    "# valid characters set\n",
    "valid_chars = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?;:()[]{}<>/-_ \\n\")\n",
    "\n",
    "clean_content, filtered_characters = filter_invalid_characters(content, valid_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer.tokenization_chatglm import ChatGLM4Tokenizer # type: ignore\n",
    "\n",
    "tokenizer_glm4 = ChatGLM4Tokenizer('tokenizer/tokenizer.model')\n",
    "\n",
    "tokens = tokenizer_glm4._tokenize('This is a Sentence. 这是一个句子。')\n",
    "token_ids = [tokenizer_glm4._convert_token_to_id(token) for token in tokens]\n",
    "\n",
    "recover_tokens = [tokenizer_glm4._convert_id_to_token(token_id) for token_id in token_ids]\n",
    "recover_text = tokenizer_glm4.convert_tokens_to_string(recover_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_to_Chunks(content, chunk_size, overlap_size):\n",
    "    character_index = 0\n",
    "    chunks = []\n",
    "    while character_index + chunk_size <= len(content):\n",
    "        chunks.append(content[character_index : character_index + chunk_size])\n",
    "        character_index += chunk_size - overlap_size\n",
    "    chunks.append(content[character_index:])\n",
    "    return chunks\n",
    "\n",
    "chunks = Split_to_Chunks(content, 1000, 200)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, tokenizer:any, embedding_weight_path:str, chunks):\n",
    "        self.chunks = chunks\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vectors = torch.load(embedding_weight_path)\n",
    "        self.vector_db = self.create_vector_db()\n",
    "        self.chunk_nums = len(self.vector_db)\n",
    "        \n",
    "    def create_vector_db(self):\n",
    "        vector_db = []\n",
    "        for chunk in self.chunks:\n",
    "            tokens = self.tokenizer._tokenize(chunk)\n",
    "            token_ids = [self.tokenizer._convert_token_to_id(token) for token in tokens]\n",
    "            vector_db.append(self.vectors[token_ids])\n",
    "        return vector_db\n",
    "    \n",
    "    def _query_to_vectors(self, query, top_k):\n",
    "        tokens = self.tokenizer._tokenize(query)\n",
    "        token_ids = [self.tokenizer._convert_token_to_id(token) for token in tokens]\n",
    "        query_vectors = self.vectors[token_ids]\n",
    "        cosine_similarity_score = torch.ones(self.chunk_nums)\n",
    "        for i in range(self.chunk_nums):\n",
    "            cosine_similarity_score[i] = self.average_cosine_similarity(self.vector_db[i], query_vectors)\n",
    "        similarity_score, chunk_indices = torch.topk(cosine_similarity_score, top_k)\n",
    "        results = [self.vector_db[chunk_indice] for chunk_indice in chunk_indices]\n",
    "        return results, chunk_indices, similarity_score\n",
    "        \n",
    "    def _query_to_text(self, query, top_k):\n",
    "        _, chunk_indices, _ = self._query_to_vectors(query, top_k)\n",
    "        results = [self.chunks[chunk_indice] for chunk_indice in chunk_indices]\n",
    "        return results\n",
    "        \n",
    "    def average_cosine_similarity(self, chunk_vectors, query_vertors):\n",
    "        chunk_vectors_norm = chunk_vectors / chunk_vectors.norm(dim=1, keepdim=True)\n",
    "        query_vertors_norm = query_vertors / query_vertors.norm(dim=1, keepdim=True)\n",
    "        cosine_sim = torch.mm(chunk_vectors_norm, query_vertors_norm.t())  \n",
    "        return cosine_sim.mean()\n",
    "    \n",
    "VectorDB = VectorDB(tokenizer_glm4, 'tokenizer/embedding_weight.pt', chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_text = VectorDB._query_to_text('注意力', 3)\n",
    "# for text in query_text:\n",
    "#     print(text)\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented and Generation:Take the GLM4 for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "model_dir = 'GLM4CKPT'\n",
    "glm4_tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "class RAG_GLM4:\n",
    "    def __init__(self, model, tokenizer, VectorDB):\n",
    "        self.model = model\n",
    "        self.VectorDB = VectorDB\n",
    "        self.tokenizer = tokenizer\n",
    "    def generate(self, query, top_k=1):\n",
    "        response, _ = model.chat(self.tokenizer, self.augmented(query, top_k), history=[])\n",
    "        return response\n",
    "    def augmented(self, query, top_k=1):\n",
    "        prompt = f\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question: {query}. If you don't know the answer, say that you don't know. \\n\\n Retrieved context: {self.VectorDB._query_to_text(query, top_k)}.\"\n",
    "        '''\n",
    "        A Chinese Version\n",
    "        prompt = f\"你是一个问答任务的助手。使用以下检索到的文本来回答问题：{query}。如果你不知道答案，请说你不知道。\\n\\n检索到的参考文本：{self.VectorDB._query_to_text(query, top_k)}。\"\n",
    "        '''\n",
    "        return prompt\n",
    "    \n",
    "RAG_GLM4 = RAG_GLM4(model, glm4_tokenizer, VectorDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Explaining DDPM'\n",
    "response_rag = RAG_GLM4.generate(query, 3)\n",
    "print(response_rag)\n",
    "print('-'*100)\n",
    "response, _ = model.chat(glm4_tokenizer, query, history=[])\n",
    "print('\\n', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = VectorDB._query_to_text(query, 3)\n",
    "for text in query_text:\n",
    "    print(text)\n",
    "    print('-'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
