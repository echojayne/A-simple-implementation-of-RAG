{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get PDF Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in ./miniconda3/envs/RAG-GLM4/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Current Working Directory: /home/dky/RAG\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber\n",
    "import os\n",
    "os.chdir('RAG')\n",
    "print(\"Current Working Directory:\", os.getcwd()) # Mark sure your current working directory is in /path/to/RAG\n",
    "import pdfplumber # type: ignore\n",
    "\n",
    "path = 'dataset/general_notes.pdf'\n",
    "with pdfplumber.open(path) as pdf: \n",
    "    content = ''\n",
    "    for i in range(len(pdf.pages)):\n",
    "        page = pdf.pages[i] \n",
    "        page_content = '\\n'.join(page.extract_text().split('\\n')[:-1])\n",
    "        content = content + page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering invalid characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_invalid_characters(text, valid_chars):\n",
    "    \"\"\"Filter out characters that are not in the valid character set.\"\"\"\n",
    "    vilid_characters = ''.join([char for char in text if char in valid_chars])\n",
    "    filtered_characters = ''.join([char for char in text if char not in vilid_characters])\n",
    "    return vilid_characters, filtered_characters\n",
    "\n",
    "# valid characters set\n",
    "valid_chars = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?;:()[]{}<>/-_ \\n\")\n",
    "\n",
    "clean_content, filtered_characters = filter_invalid_characters(content, valid_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_chatglm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGLM4Tokenizer \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tokenizer_glm4 \u001b[38;5;241m=\u001b[39m ChatGLM4Tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer/tokenizer.model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer_glm4\u001b[38;5;241m.\u001b[39m_tokenize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is a Sentence. 这是一个句子。\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/RAG/tokenizer/tokenization_chatglm.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorType\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union, Dict, Any\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from tokenizer.tokenization_chatglm import ChatGLM4Tokenizer # type: ignore\n",
    "\n",
    "tokenizer_glm4 = ChatGLM4Tokenizer('tokenizer/tokenizer.model')\n",
    "\n",
    "tokens = tokenizer_glm4._tokenize('This is a Sentence. 这是一个句子。')\n",
    "token_ids = [tokenizer_glm4._convert_token_to_id(token) for token in tokens]\n",
    "\n",
    "recover_tokens = [tokenizer_glm4._convert_id_to_token(token_id) for token_id in token_ids]\n",
    "recover_text = tokenizer_glm4.convert_tokens_to_string(recover_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_to_Chunks(content, chunk_size, overlap_size):\n",
    "    character_index = 0\n",
    "    chunks = []\n",
    "    while character_index + chunk_size <= len(content):\n",
    "        chunks.append(content[character_index : character_index + chunk_size])\n",
    "        character_index += chunk_size - overlap_size\n",
    "    chunks.append(content[character_index:])\n",
    "    return chunks\n",
    "\n",
    "chunks = Split_to_Chunks(content, 1000, 200)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, tokenizer:any, embedding_weight_path:str, chunks):\n",
    "        self.chunks = chunks\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vectors = torch.load(embedding_weight_path)\n",
    "        self.vector_db = self.create_vector_db()\n",
    "        self.chunk_nums = len(self.vector_db)\n",
    "        \n",
    "    def create_vector_db(self):\n",
    "        vector_db = []\n",
    "        for chunk in self.chunks:\n",
    "            tokens = self.tokenizer._tokenize(chunk)\n",
    "            token_ids = [self.tokenizer._convert_token_to_id(token) for token in tokens]\n",
    "            vector_db.append(self.vectors[token_ids])\n",
    "        return vector_db\n",
    "    \n",
    "    def _query_to_vectors(self, query, top_k):\n",
    "        tokens = self.tokenizer._tokenize(query)\n",
    "        token_ids = [self.tokenizer._convert_token_to_id(token) for token in tokens]\n",
    "        query_vectors = self.vectors[token_ids]\n",
    "        cosine_similarity_score = torch.ones(self.chunk_nums)\n",
    "        for i in range(self.chunk_nums):\n",
    "            cosine_similarity_score[i] = self.average_cosine_similarity(self.vector_db[i], query_vectors)\n",
    "        similarity_score, chunk_indices = torch.topk(cosine_similarity_score, top_k)\n",
    "        results = [self.vector_db[chunk_indice] for chunk_indice in chunk_indices]\n",
    "        return results, chunk_indices, similarity_score\n",
    "        \n",
    "    def _query_to_text(self, query, top_k):\n",
    "        _, chunk_indices, _ = self._query_to_vectors(query, top_k)\n",
    "        results = [self.chunks[chunk_indice] for chunk_indice in chunk_indices]\n",
    "        return results\n",
    "        \n",
    "    def average_cosine_similarity(self, chunk_vectors, query_vertors):\n",
    "        chunk_vectors_norm = chunk_vectors / chunk_vectors.norm(dim=1, keepdim=True)\n",
    "        query_vertors_norm = query_vertors / query_vertors.norm(dim=1, keepdim=True)\n",
    "        cosine_sim = torch.mm(chunk_vectors_norm, query_vertors_norm.t())  \n",
    "        return cosine_sim.mean()\n",
    "    \n",
    "VectorDB = VectorDB(tokenizer_glm4, 'tokenizer/embedding_weight.pt', chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_text = VectorDB._query_to_text('注意力', 3)\n",
    "# for text in query_text:\n",
    "#     print(text)\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented and Generation:Take the GLM4 for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "model_dir = 'GLM4CKPT'\n",
    "glm4_tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "class RAG_GLM4:\n",
    "    def __init__(self, model, tokenizer, VectorDB):\n",
    "        self.model = model\n",
    "        self.VectorDB = VectorDB\n",
    "        self.tokenizer = tokenizer\n",
    "    def generate(self, query, top_k=1):\n",
    "        response, _ = model.chat(self.tokenizer, self.augmented(query, top_k), history=[])\n",
    "        return response\n",
    "    def augmented(self, query, top_k=1):\n",
    "        prompt = f\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question: {query}. If you don't know the answer, say that you don't know. \\n\\n Retrieved context: {self.VectorDB._query_to_text(query, top_k)}.\"\n",
    "        '''\n",
    "        A Chinese Version\n",
    "        prompt = f\"你是一个问答任务的助手。使用以下检索到的文本来回答问题：{query}。如果你不知道答案，请说你不知道。\\n\\n检索到的参考文本：{self.VectorDB._query_to_text(query, top_k)}。\"\n",
    "        '''\n",
    "        return prompt\n",
    "    \n",
    "RAG_GLM4 = RAG_GLM4(model, glm4_tokenizer, VectorDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Explaining DDPM'\n",
    "response_rag = RAG_GLM4.generate(query, 3)\n",
    "print(response_rag)\n",
    "print('-'*100)\n",
    "response, _ = model.chat(glm4_tokenizer, query, history=[])\n",
    "print('\\n', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = VectorDB._query_to_text(query, 3)\n",
    "for text in query_text:\n",
    "    print(text)\n",
    "    print('-'*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
